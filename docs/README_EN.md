<p align="center">
  <a href="../README.md">ç®€ä½“ä¸­æ–‡</a> Â·
  <a href="README_EN.md">English</a> Â·
  <a href="README_JA.md">æ—¥æœ¬èª</a>
</p>

# QSAR Prediction Model for Huggins Parameter (Ï‡) Based on Molecular Descriptors

> âš ï¸ This version was translated by AI and may contain errors.

> This project uses **QSAR (Quantitative Structure-Activity Relationship)** methods to predict the **Huggins parameter (Ï‡)** of polymer-solvent systems using molecular descriptors and ML/DNN models.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Environment Setup](#environment-setup)
- [Full Pipeline](#full-pipeline)
  - [Step 1: Obtain SMILES](#step-1-obtain-smiles-molecular-representations)
  - [Step 2: Data Preprocessing](#step-2-data-preprocessing)
  - [Step 2.5: Dataset Merging](#step-25-dataset-merging)
  - [Step 3: Feature Engineering](#step-3-feature-engineering)
  - [Step 4: Feature Selection](#step-4-two-stage-feature-selection)
  - [Step 5: Model Training & AutoTuning](#step-5-model-training--auto-tuning)
  - [Step 6: Model Validation & Analysis](#step-6-model-validation--analysis)
- [Data Files](#data-files)
- [Model Performance Benchmarks](#model-performance-benchmarks)
- [Dependencies](#dependencies)

---

## Overview

The **Huggins parameter (Ï‡)** is a key thermodynamic parameter describing polymer-solvent interactions, reflecting the affinity between solvent and polymer in a mixed system.

Core workflow:

1. Extract compound names from literature data and convert to **SMILES** molecular representations
2. Merge multi-source datasets (old: 323 + new: 1586 = **1,893 samples**)
3. Compute all **~210** 2D molecular descriptors using **RDKit**, plus fingerprint similarities and interaction features â†’ **320-dimensional feature matrix**
4. Use **Genetic Algorithm (GA)** to select optimal feature subset from 320 dimensions
5. Train ML/DNN models with **AutoTune** hyperparameter optimization using selected features

---

## Project Structure

```
Graduation-project/
â”‚
â”œâ”€â”€ è·å–SMILES.py              # Step 1: Compound name â†’ SMILES
â”œâ”€â”€ æ•°æ®å¤„ç†éƒ¨åˆ†ä»£ç .py          # Step 2: Ï‡ expression parsing + temperature expansion
â”œâ”€â”€ åˆå¹¶æ•°æ®é›†.py               # Step 2.5: Merge old and new datasets
â”œâ”€â”€ ç‰¹å¾å·¥ç¨‹.py                 # Step 3: Full RDKit descriptor extraction (320-dim)
â”œâ”€â”€ ç‰¹å¾ç­›é€‰.py                 # Step 4a: RFECV feature selection
â”œâ”€â”€ é—ä¼ .py                    # Step 4b: Genetic Algorithm (GA) feature selection
â”œâ”€â”€ feature_config.py           # Feature config center (unified feature column management)
â”‚
â”œâ”€â”€ DNN.py                     # Step 5a: DNN deep neural network modeling
â”œâ”€â”€ DNN_AutoTune.py            # Step 5b: DNN Hyperband auto-tuning
â”œâ”€â”€ Sklearn.py                 # Step 5c: Sklearn Bayesian optimization modeling
â”œâ”€â”€ Sklearn_AutoTune.py        # Step 5d: Sklearn RandomizedSearch auto-tuning
â”‚
â”œâ”€â”€ DNN_æ¨¡å‹éªŒè¯.py             # Step 6a: DNN model validation
â”œâ”€â”€ DNNç‰¹å¾è´¡çŒ®åˆ†æ.py          # Step 6c: DNN SHAP feature contribution analysis
â”œâ”€â”€ Y_Randomization.py         # Step 6d: Y-Randomization (Y-Scrambling) test
â”‚
â”œâ”€â”€ Huggins.xlsx               # Raw data: compound names + Huggins parameters
â”‚
â”œâ”€â”€ data/                      # Intermediate data
â”‚   â”œâ”€â”€ smiles_raw.csv
â”‚   â”œâ”€â”€ smiles_cleaned.xlsx
â”‚   â”œâ”€â”€ huggins_preprocessed.xlsx
â”‚   â”œâ”€â”€ 43579_2022_237_MOESM1_ESM.csv  # External dataset (1,586 entries)
â”‚   â”œâ”€â”€ merged_dataset.csv             # Merged dataset (1,893 entries)
â”‚   â”œâ”€â”€ molecular_features.xlsx        # 320-dim feature matrix
â”‚   â””â”€â”€ features_optimized.xlsx        # Selected feature subset
â”‚
â”œâ”€â”€ results/                   # Models & results
â”‚   â”œâ”€â”€ dnn_model.keras
â”‚   â”œâ”€â”€ dnn_preprocess.pkl
â”‚   â”œâ”€â”€ sklearn_model_bundle.pkl
â”‚   â”œâ”€â”€ ga_best_model.pkl
â”‚   â”œâ”€â”€ ga_selected_features.txt
â”‚   â”œâ”€â”€ ga_evolution_log.csv
â”‚   â”œâ”€â”€ sklearn_tuning_summary.csv
â”‚   â”œâ”€â”€ train_test_split_indices.npz   # Unified train/test split indices
â”‚   â”œâ”€â”€ feature_selection.png
â”‚   â””â”€â”€ dnn_loss.png
â”‚
â”œâ”€â”€ final_results/             # Final deliverables (separated from intermediates)
â”‚   â””â”€â”€ sklearn/
â”‚       â”œâ”€â”€ sklearn_model_bundle.pkl
â”‚       â”œâ”€â”€ fingerprint_model.pkl
â”‚       â”œâ”€â”€ sklearn_tuning_summary.csv
â”‚       â”œâ”€â”€ sklearn_validation_results.xlsx
â”‚       â”œâ”€â”€ sklearn_feature_importance.csv
â”‚       â”œâ”€â”€ sklearn_feature_importance.png
â”‚       â”œâ”€â”€ sklearn_validation_plots.png
â”‚       â”œâ”€â”€ y_randomization.png
â”‚       â”œâ”€â”€ y_randomization.csv
â”‚       â””â”€â”€ sklearn_final_report.txt
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Environment Setup

### Prerequisites

- Python 3.8+
- pip package manager

### Install Dependencies

```bash
pip install -r requirements.txt
conda install -c conda-forge rdkit  # RDKit must be installed via conda
```

### Key Dependencies

| Library | Purpose |
|---------|---------|
| `pandas` / `numpy` | Data processing & scientific computing |
| `rdkit` | Molecular descriptor computation, fingerprint generation |
| `scikit-learn` | Traditional ML models & data preprocessing |
| `scikit-optimize` | Bayesian hyperparameter optimization (BayesSearchCV) |
| `xgboost` | XGBoost regression model |
| `deap` | Genetic Algorithm feature selection |
| `tensorflow` / `keras` | Deep Neural Network (DNN) |
| `keras-tuner` | DNN Hyperband auto-tuning |
| `shap` | Model interpretability analysis (SHAP values) |
| `joblib` | Model serialization |
| `matplotlib` | Data visualization |
| `requests` / `tqdm` | HTTP requests / progress bars |

---

## Full Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Complete Pipeline                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Huggins.xlsx â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚       â”‚                        â”‚                                    â”‚
â”‚       â–¼                        â”‚                                    â”‚
â”‚  Step 1: è·å–SMILES.py         â”‚                                    â”‚
â”‚       â”‚                        â”‚                                    â”‚
â”‚       â–¼                        â”‚                                    â”‚
â”‚  Step 2: æ•°æ®å¤„ç†éƒ¨åˆ†ä»£ç .py   â”‚                                    â”‚
â”‚       â”‚                        â”‚                                    â”‚
â”‚       â–¼                        â–¼                                    â”‚
â”‚  Step 2.5: åˆå¹¶æ•°æ®é›†.py â—„â”€â”€â”€ New data (ESM.csv)                    â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼                                                             â”‚
â”‚  Step 3: ç‰¹å¾å·¥ç¨‹.py â†’ 320-dim full RDKit descriptors               â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼                                                             â”‚
â”‚  Step 4a: é—ä¼ .py (GA coarse: 320 â†’ ~20-40)                        â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼                                                             â”‚
â”‚  Step 4b: ç‰¹å¾ç­›é€‰.py (RFECV fine: ~20-40 â†’ ~8-15)                  â”‚
â”‚       â”‚                                                             â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚       â–¼                     â–¼                                       â”‚
â”‚  Step 5a: Sklearn       Step 5b: DNN                                â”‚
â”‚  (Sklearn_AutoTune.py)  (DNN.py / DNN_AutoTune.py)                  â”‚
â”‚       â”‚                     â”‚                                       â”‚
â”‚       â–¼                     â–¼                                       â”‚
â”‚  Step 6: Validation + Feature Contribution Analysis                 â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Step 1: Obtain SMILES Molecular Representations

**Script**: [`è·å–SMILES.py`](è·å–SMILES.py)

**Function**: Converts compound names from `Huggins.xlsx` to SMILES molecular structure strings via PubChem / NCI API.

```bash
python è·å–SMILES.py
```

> âš ï¸ Requires internet access to query PubChem and NCI databases.

---

### Step 2: Data Preprocessing

**Script**: [`æ•°æ®å¤„ç†éƒ¨åˆ†ä»£ç .py`](æ•°æ®å¤„ç†éƒ¨åˆ†ä»£ç .py)

**Function**: Handles temperature-dependent Ï‡ expressions (e.g., `0.43+34.7T`), temperature expansion (20-50Â°C), and outlier filtering (`-1 < Ï‡ < 5`).

```bash
python æ•°æ®å¤„ç†éƒ¨åˆ†ä»£ç .py
```

---

### Step 2.5: Dataset Merging

**Script**: [`åˆå¹¶æ•°æ®é›†.py`](åˆå¹¶æ•°æ®é›†.py)

**Function**: Merges old data (`huggins_preprocessed.xlsx`, 323 entries) with new external data (`43579_2022_237_MOESM1_ESM.csv`, 1,586 entries) into a unified format. After deduplication: **1,893 samples**.

**Data flow**: Old data + New data â†’ `data/merged_dataset.csv`

**Unified columns**: `Polymer, Solvent, Polymer_SMILES, Solvent_SMILES, chi, temperature, source`

```bash
python åˆå¹¶æ•°æ®é›†.py
```

---

### Step 3: Feature Engineering

**Script**: [`ç‰¹å¾å·¥ç¨‹.py`](ç‰¹å¾å·¥ç¨‹.py)

**Function**: Uses RDKit's `CalcMolDescriptors()` to extract all **~210 2D molecular descriptors** for both polymer and solvent, then adds fingerprint similarity and interaction features.

**Data flow**: `data/merged_dataset.csv` â†’ `data/molecular_features.xlsx`

| Feature Category | Count | Description |
|-----------------|-------|-------------|
| Polymer descriptors (suffix `_1`) | ~148 | MolWt, LogP, TPSA, fragment counts, topological indices, etc. |
| Solvent descriptors (suffix `_2`) | ~155 | Same as above |
| Fingerprint similarity | 3 | Avalon, Morgan, Topological |
| Interaction features | 14 | Delta_LogP, Delta_TPSA, HB_Match, Inv_T, etc. |
| **Total** | **~320** | After cleaning (removing high-missing + constant columns) |

**Special handling**: `[*]` connection point markers in polymer SMILES are replaced with `[H]` for proper RDKit parsing.

```bash
python ç‰¹å¾å·¥ç¨‹.py
```

---

### Step 4: Two-Stage Feature Selection

Uses a **GA coarse screening â†’ RFECV fine screening** two-stage strategy to progressively select optimal features from 320 dimensions:

```
320-dim â”€â”€GA coarseâ”€â”€â†’ ~20-40 dim â”€â”€RFECV fineâ”€â”€â†’ ~8-15 dim â”€â”€â†’ Modeling
```

#### Step 4a: Genetic Algorithm (GA) Coarse Screening

**Script**: [`é—ä¼ .py`](é—ä¼ .py)

**Function**: Uses DEAP genetic algorithm to globally search for optimal feature subsets from ~320 dimensions. GA can explore nonlinear feature combination effects, suitable for high-dimensional coarse screening.

| Parameter | Value | Description |
|-----------|-------|-------------|
| Population size | 100 | 100 candidates per generation |
| Max generations | 60 | Upper limit (usually early-stopped) |
| Early stopping | 12 generations no improvement | Automatic stop |
| CV folds | 3 | Balance speed and accuracy |
| Estimator | RF(n=100, depth=8) | Lightweight and fast |
| Feature count constraint | [5, 40] | Control model complexity |

**Output**: `results/ga_selected_features.txt`, `results/ga_evolution_log.csv`, `results/train_test_split_indices.npz`, auto-updates `feature_config.py`

> â„¹ï¸ GA creates and saves train/test split indices. All downstream scripts automatically reuse the same split, ensuring complete test set isolation.

```bash
python é—ä¼ .py    # ~20-40 minutes
```

#### Step 4b: RFECV Fine Screening

**Script**: [`ç‰¹å¾ç­›é€‰.py`](ç‰¹å¾ç­›é€‰.py)

**Function**: From GA-selected ~20-40 features, uses RFECV to iteratively remove redundant features and pinpoint the optimal subset. Automatically reads GA results from `feature_config.py`.

> âš ï¸ Must run `é—ä¼ .py` first. Automatically loads GA-saved train/test split indices and performs selection only on the training set.

**Output**: Auto-updates `feature_config.py` and `data/features_optimized.xlsx`

```bash
python ç‰¹å¾ç­›é€‰.py
```

#### Unified Feature Management

**Script**: [`feature_config.py`](feature_config.py)

Feature selection results are stored in this file, defining `SELECTED_FEATURE_COLS` (selected features) for use by downstream training and validation scripts.

---

### Step 5: Model Training & Auto-Tuning

#### Step 5a: DNN Deep Neural Network

**Script**: [`DNN.py`](DNN.py)

| Config | Value |
|--------|-------|
| Architecture | 48 â†’ BN â†’ Dropout(0.15) â†’ 24 â†’ BN â†’ Dropout(0.1) â†’ 12(L2) â†’ 1 |
| Loss function | Huber |
| Training strategy | Train with 5 random seeds, select best |
| Data split | 60% train / 20% validation / 20% test |
| Normalization | StandardScaler on both X and y |

```bash
.venv\Scripts\python.exe DNN.py
```

#### Step 5b: DNN Hyperband Auto-Tuning

**Script**: [`DNN_AutoTune.py`](DNN_AutoTune.py)

Uses Keras Tuner's Hyperband algorithm to search for optimal DNN architecture (1-3 layers, 12-64 units, learning rate, regularization, etc.).

```bash
.venv\Scripts\python.exe DNN_AutoTune.py
```

#### Step 5c: Sklearn Traditional Machine Learning

**Script**: [`Sklearn.py`](Sklearn.py)

Batch trains multiple Sklearn regression models using BayesSearchCV for optimal parameter search.

#### Step 5d: Sklearn AutoTune (Recommended)

**Script**: [`Sklearn_AutoTune.py`](Sklearn_AutoTune.py)

4 models Ã— 50 parameter sets Ã— 5-fold CV automatic optimization:

| Model | Search Dimensions |
|-------|-------------------|
| GradientBoosting | loss, lr, n_estimators, depth, subsample |
| XGBRegressor | lr, n_estimators, depth, reg_alpha/lambda |
| RandomForest | n_estimators, depth, max_features |
| MLPRegressor | hidden layers, activation, alpha, lr |

After execution, automatically completes:

1. Best model search (CV model selection)
2. Test set validation (RÂ²/MAE/RMSE, using only unseen test data)
3. Feature contribution analysis (built-in importance or permutation importance)
4. Validation visualization (Actual vs Predicted, residual distribution, model comparison â€” 4 plots)
5. Final deliverables output to `final_results/sklearn/`

```bash
python Sklearn_AutoTune.py
```

---

### Step 6: Model Validation & Analysis

#### Model Validation

| Script | Function |
|--------|----------|
| [`DNN_æ¨¡å‹éªŒè¯.py`](DNN_æ¨¡å‹éªŒè¯.py) | Load DNN model and evaluate RÂ²/MAE/RMSE on full data |
| [`Sklearn_AutoTune.py`](Sklearn_AutoTune.py) | Automatically outputs Sklearn validation results after training (`final_results/sklearn/sklearn_validation_results.xlsx`) |

#### Feature Contribution Analysis

| Script | Function |
|--------|----------|
| [`DNNç‰¹å¾è´¡çŒ®åˆ†æ.py`](DNNç‰¹å¾è´¡çŒ®åˆ†æ.py) | SHAP GradientExplainer for DNN feature contributions |
| [`Sklearn_AutoTune.py`](Sklearn_AutoTune.py) | Automatically outputs Sklearn feature importance after training (`final_results/sklearn/sklearn_feature_importance.*`) |

#### Y-Randomization Test

**Script**: [`Y_Randomization.py`](Y_Randomization.py)

**Function**: Y-Scrambling validation â€” shuffles y values 100 times and retrains the model to verify whether the QSAR model truly learned feature-target relationships. If real model RÂ² is significantly higher than randomized distribution (p < 0.05), the model is valid.

**Output**: `final_results/sklearn/y_randomization.png`, `y_randomization.csv`

```bash
python Y_Randomization.py
```

---

## Data Files

| File | Location | Description | Stage |
|------|----------|-------------|-------|
| `Huggins.xlsx` | Root | Raw data | Input |
| `43579_2022_237_MOESM1_ESM.csv` | `data/` | External dataset (1,586 entries) | Input |
| `smiles_raw.csv` | `data/` | SMILES query results | Step 1 |
| `smiles_cleaned.xlsx` | `data/` | Manually cleaned SMILES | Manual |
| `huggins_preprocessed.xlsx` | `data/` | Preprocessed data (323 entries) | Step 2 |
| `merged_dataset.csv` | `data/` | Merged dataset (1,893 entries) | Step 2.5 |
| `molecular_features.xlsx` | `data/` | 320-dim feature matrix | Step 3 |
| `features_optimized.xlsx` | `data/` | Selected feature subset | Step 4 |
| `ga_selected_features.txt` | `results/` | GA-selected feature list | Step 4a |
| `ga_evolution_log.csv` | `results/` | GA evolution log | Step 4a |
| `sklearn_model_bundle.pkl` | `results/` | Sklearn unified model bundle | Step 5 |
| `dnn_model.keras` | `results/` | DNN model | Step 5 |
| `train_test_split_indices.npz` | `results/` | Unified train/test split indices | Step 4a |
| `sklearn_final_report.txt` | `final_results/sklearn/` | Sklearn final report | Step 5d |
| `sklearn_validation_results.xlsx` | `final_results/sklearn/` | Sklearn validation details | Step 5d |
| `sklearn_feature_importance.png` | `final_results/sklearn/` | Sklearn feature importance plot | Step 5d |
| `sklearn_validation_plots.png` | `final_results/sklearn/` | Sklearn validation plots (4 subplots) | Step 5d |
| `y_randomization.png` | `final_results/sklearn/` | Y-Randomization RÂ² distribution | Step 6 |
| `y_randomization.csv` | `final_results/sklearn/` | Y-Randomization detailed data | Step 6 |

---

## Model Performance Benchmarks

> Results from AutoTune on merged dataset (1,886 samples, 6 features via RFECV)

| Model | CV Val RÂ² | Test RÂ² | Test MAE | Test RMSE |
|-------|-----------|---------|----------|-----------|
| **GradientBoosting** | **0.749** | **0.812** | 0.156 | 0.263 |
| XGBRegressor | 0.726 | 0.799 | 0.150 | 0.271 |
| RandomForest | 0.692 | 0.780 | 0.177 | 0.284 |
| MLPRegressor | 0.616 | 0.725 | 0.208 | 0.318 |
| DNN (Keras) | â€” | 0.649 | 0.240 | 0.359 |

> â„¹ï¸ All models are evaluated on the same test set. The test set does not participate in feature selection or model training.
> ğŸ’¡ Performance is expected to improve further after GA selects the optimal feature subset from 320 dimensions.

---

## Quick Start

```bash
# 1. Clone the project
git clone <repository-url>
cd Graduation-project

# 2. Install dependencies
pip install -r requirements.txt
conda install -c conda-forge rdkit

# 3. Dataset merging + Feature engineering + Two-stage feature selection + Modeling
python åˆå¹¶æ•°æ®é›†.py              # Merge old and new data
python ç‰¹å¾å·¥ç¨‹.py                # Full RDKit descriptors (320-dim)
python é—ä¼ .py                   # GA coarse screening (320 â†’ ~20-40, ~20-40 min)
python ç‰¹å¾ç­›é€‰.py                # RFECV fine screening (~20-40 â†’ ~8-15)
python Sklearn_AutoTune.py       # Sklearn auto-tuning

# Or: if you already have data/molecular_features.xlsx, start from Step 4
python é—ä¼ .py
python Sklearn_AutoTune.py
```

---

## Evaluation Metrics

| Metric | Formula | Description |
|--------|---------|-------------|
| **RÂ²** | 1 - SS_res/SS_tot | Coefficient of determination (closer to 1 = better) |
| **MAE** | mean(\|y_true - y_pred\|) | Mean Absolute Error |
| **RMSE** | âˆš(mean((y_true - y_pred)Â²)) | Root Mean Squared Error |

---

## License

This project is a graduation thesis project, for academic research purposes only.
