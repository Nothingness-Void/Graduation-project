import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import sklearn.model_selection as _ms
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import sys
import joblib
import sklearn.utils
from multiprocessing import freeze_support

# Compatibility shim for genetic_selection on newer scikit-learn
if not hasattr(sklearn.utils, "_joblib"):
    sklearn.utils._joblib = joblib
    sys.modules["sklearn.utils._joblib"] = joblib

# Compatibility shim: genetic_selection passes fit_params to cross_val_score
if not hasattr(_ms.cross_val_score, "_ga_compat"):
    _orig_cvs = _ms.cross_val_score

    def _cross_val_score_compat(*args, **kwargs):
        kwargs.pop("fit_params", None)
        return _orig_cvs(*args, **kwargs)

    _cross_val_score_compat._ga_compat = True
    _ms.cross_val_score = _cross_val_score_compat

from genetic_selection import GeneticSelectionCV

def main():
    # 1. è¯»å–ç»è¿‡æ¸…æ´—çš„æ•°æ® (ä½¿ç”¨ä½ ä¹‹å‰ç­›é€‰å‡ºçš„16ä¸ªç‰¹å¾)
    # å»ºè®®ç›´æŽ¥è¯»å– features_optimized.xlsxï¼Œæˆ–è€…æ‰‹åŠ¨æŒ‡å®šé‚£16ä¸ªåˆ—
    feature_cols = [
        'MolWt1', 'logP1', 'TPSA1', 'MaxAbsPartialCharge1', 'LabuteASA1',
        'logP2', 'MaxAbsPartialCharge2', 'LabuteASA2',
        'Avalon Similarity', 'Morgan Similarity',
        'Delta_LogP', 'Delta_TPSA', 'HB_Match', 'Delta_MolMR',
        'CSP3_2', 'Inv_T'
    ]

    data = pd.read_excel('data/features_optimized.xlsx')
    X = data[feature_cols]
    y = data['Ï‡-result']

    # åˆ’åˆ†æ•°æ® (ä¿æŒå’Œä¹‹å‰ä¸€æ ·çš„éšæœºç§å­ï¼Œæ–¹ä¾¿å¯¹æ¯”)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # æ ‡å‡†åŒ– (GA å¯¹æ•°å€¼æ•æ„Ÿï¼Œå¿…é¡»æ ‡å‡†åŒ–)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ============================================================
    # æ ¸å¿ƒéƒ¨åˆ†ï¼šé…ç½®é—ä¼ ç®—æ³•
    # ============================================================
    # è¿™é‡Œçš„ estimator å¯ä»¥æ¢ã€‚
    # ç­–ç•¥ A: LinearRegression -> å¯»æ‰¾æœ€ç®€å•çš„ç‰©ç†å…¬å¼ (GA-MLR) -> è§£é‡Šæ€§æœ€å¼º
    # ç­–ç•¥ B: RandomForest -> å¯»æ‰¾æœ€å¼ºçš„éžçº¿æ€§ç»„åˆ -> ç²¾åº¦æœ€é«˜
    estimator = LinearRegression()
    # estimator = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)
    clf = RandomForestRegressor(
    n_estimators=50,   # è¿›åŒ–é˜¶æ®µ 50 æ£µæ ‘è¶³å¤Ÿäº†ï¼ŒèŠ‚çœè®¡ç®—æ—¶é—´
    max_depth=5,       # é™åˆ¶æ·±åº¦é˜²æ­¢ç‰¹å¾é€‰æ‹©é˜¶æ®µå°±è¿‡æ‹Ÿåˆ
    n_jobs=-1,
    random_state=42
)
    print(f"æ­£åœ¨å¯åŠ¨é—ä¼ ç®—æ³•è¿›åŒ–ï¼Œä½¿ç”¨æ¨¡åž‹: {estimator.__class__.__name__} ...")
    print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ç”Ÿç‰©è¿›åŒ–...")

    selector = GeneticSelectionCV(
        clf,
        cv=5,
        verbose=1,
        scoring="r2",
        max_features=10,  # é™åˆ¶æœ€å¤šé€‰10ä¸ªç‰¹å¾ (é˜²æ­¢è¿‡æ‹Ÿåˆ)
        n_population=200, # ç§ç¾¤å¤§å°ï¼šä¸€æ¬¡å…»200ä¸ªæ¨¡åž‹
        crossover_proba=0.5, # æ‚äº¤çŽ‡
        mutation_proba=0.2,  # å˜å¼‚çŽ‡ (é‡è¦ï¼é˜²æ­¢è¿‘äº²ç¹æ®–)
        n_generations=50, # è¿›åŒ–ä»£æ•°ï¼šç¹è¡50ä»£
        crossover_independent_proba=0.5,
        mutation_independent_proba=0.05,
        tournament_size=3,
        n_gen_no_change=10, # å¦‚æžœ10ä»£æ²¡æœ‰è¿›åŒ–ï¼Œæå‰ç»“æŸ
        caching=True,
        n_jobs=-1
    )

    # å¼€å§‹è¿›åŒ–
    selector = selector.fit(X_train_scaled, y_train)

    # ============================================================
    # ç»“æžœåˆ†æž
    # ============================================================

    # èŽ·å–è¢«é€‰ä¸­çš„ç‰¹å¾
    selected_features = X.columns[selector.support_]
    print("\n" + "="*50)
    print("ðŸŽ‰ è¿›åŒ–å®Œæˆï¼è‡ªç„¶é€‰æ‹©çš„ç»“æžœï¼š")
    print("="*50)
    print(f"ä¿ç•™äº† {len(selected_features)} ä¸ªç‰¹å¾ï¼š")
    print(list(selected_features))

    # åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯
    # æ³¨æ„ï¼šå¿…é¡»åªç”¨é€‰å‡ºæ¥çš„ç‰¹å¾åŽ»é¢„æµ‹
    X_train_sel = selector.transform(X_train_scaled)
    X_test_sel = selector.transform(X_test_scaled)

    # é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡åž‹
    estimator.fit(X_train_sel, y_train)
    y_pred = estimator.predict(X_test_sel)

    final_r2 = r2_score(y_test, y_pred)
    print(f"\næœ€ç»ˆæ¨¡åž‹ Test R2: {final_r2:.4f}")

    # å¦‚æžœä½ ç”¨çš„æ˜¯çº¿æ€§å›žå½’ï¼Œè¿˜å¯ä»¥æ‰“å°å‡ºå…¬å¼
    if isinstance(estimator, LinearRegression):
        print("\næŽ¨å¯¼å‡ºçš„ç‰©ç†å…¬å¼ï¼š")
        formula = "Ï‡ â‰ˆ {:.4f}".format(estimator.intercept_)
        for weight, feat in zip(estimator.coef_, selected_features):
            sign = "+" if weight >= 0 else "-"
            formula += f" {sign} {abs(weight):.4f}*{feat}"
        print(formula)


if __name__ == '__main__':
    freeze_support()
    main()